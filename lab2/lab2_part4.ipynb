{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"logo/spark.png\" alt=\"Hadoop Logo\" width=\"250\"/></p>\n",
    "# **Lab 2 - Part 4 - Spark SQL**\n",
    "#### Spark SQL is a Spark module for structured data processing. It provides a programming abstraction called `DataFrames` and can also act as distributed SQL query engine. This lab presents how to work with Spark SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 1: Create a DataFrame and apply DataFrame Operation **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entry point into all relational functionality in Spark is the `SQLContext` class, or one of its decedents. To create a basic `SQLContext`, all you need is a `SparkContext`. With a `SQLContext`, applications can create `DataFrames` from an existing RDD, from a Hive table, or from data sources. As an example, the following creates a `DataFrame` based on the content of a JSON file, located at `data/people/people.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SQLContext\n",
    "\n",
    "val sqlContext = new SQLContext(sc)\n",
    "\n",
    "val df = sqlContext.read.json(\"data/people/people.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try more functions on `DataFrame`. First print the schema in a tree format, using the `printSchema` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "df.<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the `select` method and print only the `name` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "// Select only the \"name\" column\n",
    "df.<FILL IN>.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use again the `select` method and select everybody, but increment the age by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "// Select everybody, but increment the age by 1\n",
    "df.<FILL IN>.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next select people older that 21, by using the `filter` method, and then count people by age. You can use `groupBy` for the second task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "// Select people older than 21\n",
    "df.<FILL IN>.show()\n",
    "\n",
    "// Count people by age\n",
    "df.<FILL IN>.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sql` function on a `SQLContext` enables applications to run SQL queries programmatically and returns the result as a `DataFrame`. Let's select everybody again with the help of the `sql` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "df.registerTempTable(\"df\")\n",
    "sqlContext.sql(<FILL IN>).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 2: Interoperating with RDDs **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL supports two different methods for converting existing RDDs into DataFrames:\n",
    " + (i) The first method uses reflection to infer the schema of an RDD that contains specific types of objects. This reflection based approach leads to more concise code and works well when you already know the schema while writing your Spark application.\n",
    " + (ii) The second method for creating `DataFrames` is through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD. While this method is more verbose, it allows you to construct `DataFrames` when the columns and their types are not known until runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's first try the inferring the schema using reflection. Spark SQL can convert an RDD of `Row` objects to a `DataFrame`, inferring the datatypes. Rows are constructed by passing a list of key/value pairs as kwargs to the `Row` class. The keys of this list define the column names of the table, and the types are inferred by looking at the first row. Since we currently only look at the first row, it is important that there is no missing data in the first row of the RDD. In future versions we plan to more completely infer the schema by looking at more data, similar to the inference that is performed on JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// TODO: Replace <FILL IN> with appropriate code\n",
    "case class Person(name: String, age: Int)\n",
    "\n",
    "// Load a text file and convert each line to a Row.\n",
    "val lines = sc.textFile(\"data/people/people.txt\")\n",
    "val parts = lines.map(l => l.split(\",\"))\n",
    "val people = parts.map(p => Person(p(0), p(1).trim.toInt))\n",
    "\n",
    "\n",
    "// Infer the schema, and register the DataFrame as a table.\n",
    "val schemaPeople = sqlContext.<FILL IN>\n",
    "schemaPeople.<FILL IN>\n",
    "\n",
    "// SQL can be run over DataFrames that have been registered as a table. Complete the following query\n",
    "// to return teenagers, i.e., age >= 13 and age <= 19.\n",
    "val teenagers = sqlContext.sql(\"SELECT name FROM people WHERE <FILL IN>\")\n",
    "\n",
    "// The results of SQL queries are DataFrames and support all the normal RDD operations.\n",
    "// The columns of a row in the result can be accessed by field index:\n",
    "teenagers.map(t => \"Name: \" + t(0)).collect().foreach(println)\n",
    "\n",
    "// or by field name:\n",
    "teenagers.map(t => \"Name: \" + t.<FILL IN>).collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When case classes cannot be defined ahead of time (for example, the structure of records is encoded in a string, or a text dataset will be parsed and fields will be projected differently for different users), a DataFrame can be created programmatically with three steps:\n",
    "* (i) Create an RDD of Rows from the original RDD.\n",
    "* (ii) Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step 1.\n",
    "* (iii) Apply the schema to the RDD of Rows via createDataFrame method provided by SQLContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Just run this code\n",
    "// Create an RDD\n",
    "val people = sc.textFile(\"data/people/people.txt\")\n",
    "\n",
    "// The schema is encoded in a string\n",
    "val schemaString = \"name age\"\n",
    "\n",
    "// Import Row.\n",
    "import org.apache.spark.sql.Row;\n",
    "\n",
    "// Import Spark SQL data types\n",
    "import org.apache.spark.sql.types.{StructType,StructField,StringType};\n",
    "\n",
    "// Generate the schema based on the string of schema\n",
    "val schema =\n",
    "  StructType(\n",
    "    schemaString.split(\" \").map(fieldName => StructField(fieldName, StringType, true)))\n",
    "\n",
    "// Convert records of the RDD (people) to Rows.\n",
    "val rowRDD = people.map(_.split(\",\")).map(p => Row(p(0), p(1).trim))\n",
    "\n",
    "// Apply the schema to the RDD.\n",
    "val peopleDataFrame = sqlContext.createDataFrame(rowRDD, schema)\n",
    "\n",
    "// Register the DataFrames as a table.\n",
    "peopleDataFrame.registerTempTable(\"people\")\n",
    "\n",
    "// SQL statements can be run by using the sql methods provided by sqlContext.\n",
    "val results = sqlContext.sql(\"SELECT name FROM people\")\n",
    "\n",
    "// The results of SQL queries are DataFrames and support all the normal RDD operations.\n",
    "// The columns of a row in the result can be accessed by field index or by field name.\n",
    "results.map(t => \"Name: \" + t(0)).collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ** Part 3: Data Sources **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL supports operating on a variety of data sources through the `DataFrame` interface. A `DataFrame` can be operated on as normal RDDs and can also be registered as a temporary table. Registering a DataFrame as a table allows you to run SQL queries over its data. In the simplest form, the default data source (*parquet* unless otherwise configured by `spark.sql.sources.default`) will be used for all operations. Save operations also can optionally take a SaveMode, that specifies how to handle existing data if present. It can take the values: `error` (default), `append`, `overwrite`, `ignore`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Just run this code\n",
    "// Load data from a parquet file\n",
    "val df = sqlContext.read.load(\"data/people/people.parquet\")\n",
    "df.select(\"name\", \"favorite_color\").write.mode(\"overwrite\").save(\"namesAndFavColors.parquet\")\n",
    "\n",
    "// Manually specify the data source type, e.g., json, parquet, jdbc.\n",
    "val jdf = sqlContext.read.format(\"json\").load(\"data/people/people.json\")\n",
    "jdf.select(\"name\", \"age\").write.format(\"parquet\").mode(\"overwrite\").save(\"namesAndAges.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Parquet* is a columnar format that is supported by many other data processing systems. Spark SQL provides support for both reading and writing Parquet files that automatically preserves the schema of the original data. Let's load data programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Just run this code\n",
    "// The RDD is implicitly converted to a DataFrame by implicits, allowing it to be stored using Parquet.\n",
    "schemaPeople.write.parquet(\"people.parquet\")\n",
    "\n",
    "// Read in the parquet file created above.  Parquet files are self-describing so the schema is preserved.\n",
    "// The result of loading a Parquet file is also a DataFrame.\n",
    "val parquetFile = sqlContext.read.parquet(\"people.parquet\")\n",
    "\n",
    "//Parquet files can also be registered as tables and then used in SQL statements.\n",
    "parquetFile.registerTempTable(\"parquetFile\")\n",
    "val teenagers = sqlContext.sql(\"SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\")\n",
    "teenagers.map(t => \"Name: \" + t(0)).collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL can automatically infer the schema of a JSON dataset and load it as a `DataFrame`. This conversion can be done using `SQLContext.read.json` on a JSON file. Note that the file that is offered as a json file is not a typical JSON file. Each line must contain a separate, self-contained valid JSON object. As a consequence, a regular multi-line JSON file will most often fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Just run this code\n",
    "// A JSON dataset is pointed to by path.\n",
    "// The path can be either a single text file or a directory storing text files.\n",
    "val people = sqlContext.read.json(\"data/people/people.json\")\n",
    "\n",
    "// The inferred schema can be visualized using the printSchema() method.\n",
    "people.printSchema()\n",
    "// root\n",
    "//  |-- age: integer (nullable = true)\n",
    "//  |-- name: string (nullable = true)\n",
    "\n",
    "// Register this DataFrame as a table.\n",
    "people.registerTempTable(\"people\")\n",
    "\n",
    "// SQL statements can be run by using the sql methods provided by sqlContext.\n",
    "val teenagers = sqlContext.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "// Alternatively, a DataFrame can be created for a JSON dataset represented by\n",
    "// an RDD[String] storing one JSON object per string.\n",
    "val anotherPeopleRDD = sc.parallelize(\n",
    "  \"\"\"{\"name\":\"Yin\",\"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"}}\"\"\" :: Nil)\n",
    "val anotherPeople = sqlContext.read.json(anotherPeopleRDD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
